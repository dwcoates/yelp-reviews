#+HTML_HEAD: <link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
#+TITLE: *@@html:<font color = "C2492F">@@Sensibility of Yelp Rating Trends@@html:</font>@@*

#+OPTIONS: toc:2 num:nil

* Initializing data 
Data has been preprocessed by a small Python script that flattens the source
json into csv and trims off useless data.
** Libraries :noexport:
Necessary libraries
#+BEGIN_SRC R :session :exports none :results none
  library(ggplot2)
  library(data.table)
  library(dplyr)
  library(ascii)
  options(asciiType = "org")
  options(max.print = 200)
#+END_SRC
** Data :ignore:
Need to load up about 2.3 gigabytes of tabular data and keep track of read
time. Thankfully ~fread~ from the the ~data.table~ package makes this process an
order of magnitude or two faster than R Base's ~read.csv~:
#+BEGIN_SRC R :session :exports none :cache 
  read_table <- function(filename) {                                          
      table <- fread(filename)  # use fread to quickly read csv file
      # Make sure there ren't any unacceptable chracters in the column names
      names(table) <- make.names(tolower(names(table)), unique = TRUE)
      table
  }

  print("Loading reviews...")
  reviews_t = system.time(reviews <- read_table('./data/review.csv'))

  print("Loading tip...")
  tips_t = system.time(tips <- read_table("./data/tip.csv"))

  print("Loading business...")
  business_t = system.time(business <- read_table("./data/business.csv"))

  print("Loading user...")
  users_t = system.time(users <- read_table("./data/user.csv"))

  print("Loading checkin...")
  checkins_t = system.time(checkins <- read_table("./data/checkin.csv"))
#+END_SRC

#+RESULTS[a89ba1709758873becea8679fe72f2880558629e]:

#+BEGIN_SRC R :session :exports results :results org
  total_load_time <- reviews_t + tips_t + business_t + users_t + checkins_t
  sprintf("Time to load CSV data into data.frames: %.2f minutes", total_load_time["elapsed"]/60.0)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Time to load CSV data into data.frames: 0.20 minutes
#+END_SRC

* Introduction
* Methodology
* Preprocessing
Zip code info for each business would be quite helpful for doing some looking
into demographical patronage trends. However, ZIP code info is not explicitly
required of businesses when registering with Yelp, so we have to see how easily
they can be extracted from from the 'full address' data:
** ZIP codes
#+BEGIN_SRC R :session :exports none :results none
  grab_zip <- function(address) {
      as.numeric(substr(address,
                        nchar(address, keepNA = TRUE) - 4,
                        nchar(address, keepNA = TRUE)))
  }

  zips = lapply(business$full_address, grab_zip)

  business <- mutate(business, zip_codes = zips)
#+END_SRC
How many ZIPs did we get? Dang, turns out that the about 1/8 of the addresses
did not contain zipcodes easily grabbed.
#+BEGIN_SRC R :session :exports none :results org
percent_null_zips <- length(zips[is.na(zips)])/length(zips)*100

sprintf("%.2f%% of restaurants have undecipherable zip codes", percent_null_zips)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
12.61% of restaurants have undecipherable zip codes
#+END_SRC

Upon closer inspection, it is apparent that generally businesses do not provide
very good address information, and in fact, many do not provide any at
all. Luckily Yelp maintains longitude/latitude coordinates of each business for
the purpose of Google maps integration, which we can confidently affirm with the
following assertion:
#+BEGIN_SRC R :session :exports both :results org
   longs <- grep('[[:digit:]]+.[[:digit:]]*', business$longitude)
   lats <- grep('[[:digit:]]+.[[:digit:]]*', business$latitude)
   stopifnot(length(longs) == length(lats),
             length(longs) == length(business$latitude))
   print("Done.")
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Done.
#+END_SRC

While geographical coordinates are readily at hand, reverse geocoding, the
process of converting geographic coordinates to zip codes or the like, is
expensive and time consuming to do accurately. Instead, we can get away with
just looking at the price range states for individual restaurants, which is much
easier. It may be interesting to at some point put in the work/money to produce
the zip code information.
** Additional categories and misc data cleaning :ignore:
#+BEGIN_SRC R :session :exports none :results  none
  business <- merge(business, 
                    rename(aggregate(stars ~ business_id,
                                     data=reviews,
                                     FUN=mean), 
                           stars.avg = stars),
                    by='business_id')
  business <- rename(business, stars.median = stars) # for pleasant merges with `reviews`
  business$price.range <- factor(business$price.range, labels=c('Low',
                                                                'Medium Low', 
                                                                'Medium High',
                                                                'High'))
#+END_SRC
* Distribution
** Rating variance
 #+BEGIN_SRC R :session :exports code :results none :cache no
   star_variance <- merge(aggregate(stars ~ business_id,
                                    data = reviews, 
                                    FUN = var),
                          na.omit(business[,c('price.range',
                                              'stars.avg',
                                              'business_id',
                                              'review_count')]),
                          by = 'business_id')
   star_variance <- rename(star_variance, stars.var = stars)
 #+END_SRC
*** stars.var ~ price.range
   Because deriving demographics from geographical coordinates is a bit
   difficult, it might be useful to get an idea of the quality of the rating
   inconsistency by instead using restaurant price range, and not restaurant
   neighborhood median income (or the like), as a feauture of interest. We can
   aggregate the businesses by price range and average out their rating variance
   to get an idea about this:

   #+NAME: stars_pr
   #+BEGIN_SRC R :session :exports code :colnames yes 
     aggregate(stars.var ~ price.range, data = star_variance, FUN = mean)
   #+END_SRC

   #+RESULTS: stars_pr
   | price.range |        stars.var |
   |-------------+------------------|
   | Low         |  1.5834411360414 |
   | Medium Low  | 1.55586274965935 |
   | Medium High | 1.67594320395976 |
   | High        | 1.91272081281026 |

   It seems that the correlation between rating inconsistency and restaurant
   expensiveness is very small. Maybe instead of price range, we can look at
   rating average:

   #+NAME: variance_vs_rating
   #+BEGIN_SRC R :session :exports code :results org
     cor(star_variance$stars.var, star_variance$stars.avg, use='complete')
   #+END_SRC

   #+RESULTS: variance_vs_rating
   #+BEGIN_SRC org
   -0.447323849535184
   #+END_SRC

   #+BEGIN_SRC R :session :exports results :results org :var x=variance_vs_rating
     sprintf("Correlation between rating variance and rating average: %.2f", 
             as.numeric(x))
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   Correlation between rating variance and rating average: -0.45
   #+END_SRC
   
*** stars.var ~ stars.avg
   So, we see that the rating variance for restaurants correlates negatively
   with their average ratings. That is, poorly rated restaurants have a tendency
   to also have more varied ratings. A possible conclusion is that restaurant
   ratings are simply skewed positively, and therefore deviation from mean
   rating is more often bounded at 5 stars than at 1 star (4 star restaurants
   will get occaisional 2-star ratings, but of course never a 6-star rating). We
   can investigate this idea with the following histographic depicting of the
   mean restaurant rating distribution. Note that we limit ourselves to those
   businesses with at least 20 reviews:

   #+BEGIN_SRC R :session :exports results :results graphics :file ./img/R_CCa0S6lS.png 
     b <- filter(business, review_count > 20)
     g <- ggplot(data=b, aes(stars.avg))
     g + geom_histogram(breaks=seq(1,5,by=.10),
                        fill="red",
                        col="red",
                        alpha=.2) + 
         labs(title = "Distribution average business rating", 
              x = "Mean Rating",
              y = "Count")
   #+END_SRC

#+RESULTS:
[[file:./img/R_CCa0S6lS.png]]
    
  Above we see that the average restaurant rating shows significant positive
  skew, and therefore the first hypothesis seems a bit more be believable. Most
  people who have experience with the internet, and therefore have some
  familiarity with online rating systems like Yelp, probably have an intuitive
  idea about this tendency for these ratings systems to have a very strong
  positive skew. I suspect a major reason for this is perceived social pressure,
  particularly in the case of a Facebook-driven website such as Yelp ([[http://sloanreview.mit.edu/article/the-problem-with-online-ratings-2/][which is
  not a unique idea]]). For reference, the mean business rating is a rather high
  src_R[:session]{sprintf("%.2f", mean(business$stars.avg))} {{{results(=3.69=)}}} stars.

*** stars.avg ~ price.range                                          :ignore:
    In any case, we might still wonder why there exists this correlation between
    rating inconsistency and average rating, yet also no such correlation
    whatsoever between rating inconsistency and restaurant expensiveness,
    statements which we might expect, possibly naively, to be quite similar
    (i.e., more expensive restaurants are generally rated more highly). The
    reason for this lack of correlation is because this is indeed a naive
    assumption:

#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_JTmgqG9.png 
  b <- aggregate(stars.avg ~ price.range, data=business, FUN=mean)
  ggplot(b, aes(x=price.range, y=stars.avg)) + 
      geom_bar(stat='identity', color='black', fill='yellow', alpha=.2) +
      scale_y_continuous(limits = c(0, 5)) + 
      xlab('Price Range') +
      ylab('Average rating (stars)') + 
      ggtitle('Business Price Range vs Rating')
#+END_SRC

#+RESULTS:
[[file:./img/R_JTmgqG9.png]]

I suspect an explanation for this indescrepency is simply that the value to
which these ratings refer is not very well in line with what we, as consumers,
intuitively and automatically summarize them to mean. So, while as a consumer we
think of these ratings, without much actual precise consideration, as a general
measure of "goodness", with zero being un-good and five being very good, of
course. However, how do we think of these ratings when we actually contribute
them? Indeed, it might be more accurate to describe my own system as how little
my satisfaction with the restaurant deviated from what I had expected. In this way,
I've normalized my perspective on the restaurant, but without actually yielding
me any bang-for-the-buck measure. I consider this a bad and unhelpful way to contribute
my opinion on the business, but this is the way that I feel I am most naturally inclined. 
** Price distribution
The pricing makeup of our positively skewed restaurant rating distribution is
not particularly surprising:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_YzrIrkYy.png 
  # priced restaurants only
  ggplot(business[!is.na(business$price.range), ],
         aes(x=stars.avg, fill=price.range)) + geom_histogram(binwidth=.25) +
         ylab('Count') +
         xlab('Rating average (mean)') +
         labs(fill="Price Range") +
         ggtitle('Distribution of ratings by business price range')
#+END_SRC

#+RESULTS:
[[file:./img/R_YzrIrkYy.png]]

Interestingly, it seems that unpriced restaurants, i.e. restaurants for which a
price range has not yet been assigned via user concensus, are, however, not only
considerably more positively rated, but also in a seemingly linear fashion:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_vvM4L9Z2.png 
  b <- business[business$review_count > 20, ]
  ggplot(b[is.na(b$price.range),], aes(x=stars.avg)) +
      geom_histogram(binwidth=.10, color='orange', fill='orange') +
      ylab('Count') +
      xlab('Rating average (mean)') +
      labs(fill="Price Range") +
      ggtitle('Distribution of ratings for unpriced businesses by price range')
#+END_SRC

#+RESULTS:
[[file:./img/R_vvM4L9Z2.png]]

We can see clearly that there is a much more siginificant positive skew for
these unrated restaurants. This begs the question, is there a downward tendency
for restaurant ratings as their profiles mature? The fact that unrated
restaurants tend to be less those with less mature profiles is glaringly
suspicious.
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_3EqwcmXp.png 
s <- star_variance[star_variance$review_count > 20, ]
ggplot(s, aes(x=stars.var)) + geom_histogram(color='red', fill='red', binwidth=.1)
#+END_SRC

#+RESULTS:
[[file:./img/R_3EqwcmXp.png]]

#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_5QvlYFse.png 
  s <- star_variance[star_variance$review_count > 100, ]
  ggplot(s, aes(x=stars.var, y=stars.avg)) + geom_point()
#+END_SRC

#+RESULTS:
[[file:./img/R_5QvlYFse.png]]


#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_wGL1DyI7.png 
  s <- star_variance[star_variance$review_count > 100, ]
  ggplot(s, aes(x=stars.var, y=review_count)) + geom_point()
#+END_SRC

#+RESULTS:
[[file:./img/R_wGL1DyI7.png]]

*** Basic stuff
 #+BEGIN_SRC R :session :exports results :results org
   sprintf("Average rating across all reviews: %.3f", mean(reviews$stars))
 #+END_SRC

 #+RESULTS: 
 #+BEGIN_SRC org
 Average rating across all reviews: 3.764
 #+END_SRC

* BIN
We can see the law of large numbers in action
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_vQgMpNec.png 
  s <- sample_n(filter(star_variance, review_count > 30 ), 16000)
  ggplot(filter(s, as.numeric(s$price.range) == 1), aes(x=review_count, y=stars.var)) + 
      geom_point() + 
      scale_y_continuous(limits = c(0, 4)) + 
      scale_x_continuous(limits = c(0, 4000))
#+END_SRC

#+RESULTS:
[[file:./img/R_vQgMpNec.png]]


#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_8bbjovom.png 
  s <- sample_n(filter(star_variance, review_count > 30 ), 16000)
  ggplot(business, aes(x=review_count, y=stars.avg)) + 
      geom_point() + 
      scale_y_continuous(limits = c(0, 4)) + 
      scale_x_continuous(limits = c(0, 4000))
#+END_SRC

#+RESULTS:
[[file:./img/R_8bbjovom.png]]P
