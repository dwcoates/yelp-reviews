#+HTML_HEAD: <link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
#+TITLE: *@@html:<font color = "C2492F">@@Sensibility of Yelp Rating Trends@@html:</font>@@*

#+OPTIONS: toc:2 num:nil

* Core   :noexport:
[[http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/][multiplot function]]
#+BEGIN_SRC R :session :exports none :results output org 
  # Multiple plot function
  #
  # ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
  # - cols:   Number of columns in layout
  # - layout: A matrix specifying the layout. If present, 'cols' is ignored.
  #
  # If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
  # then plot 1 will go in the upper left, 2 will go in the upper right, and
  # 3 will go all the way across the bottom.
  #
  multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
    library(grid)

    # Make a list from the ... arguments and plotlist
    plots <- c(list(...), plotlist)

    numPlots = length(plots)

    # If layout is NULL, then use 'cols' to determine layout
    if (is.null(layout)) {
      # Make the panel
      # ncol: Number of columns of plots
      # nrow: Number of rows needed, calculated from # of cols
        layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                         ncol = cols, nrow = ceiling(numPlots/cols))
    }

   if (numPlots==1) {
      print(plots[[1]])

    } else {
      # Set up the page
      grid.newpage()
      pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

      # Make each plot, in the correct location
      for (i in 1:numPlots) {
        # Get the i,j matrix positions of the regions that contain this subplot
        matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

        print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                        layout.pos.col = matchidx$col))
      }
    }
  }
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
#+END_SRC

* Introduction
The ubiquity of user review services is a modern phenomenon, being perhaps one
of the most obvious and profoundly popular uses of the internet. Do you need to
know the general opinion about something? Google it. Google an interesting pair
of headphones to read its Amazon reviews, google the new Marvel movie to get
movie-goer reviews from IMDb, google the neighborhood restaurant to peruse its
Yelp reviews. Yelp reviewership is the subject of this blog post, and in
particular, a look at how reviewers tend to disbalance their rating
standards. This endeavor begins by taking a look at rating consistency by
context; the entry point being an investigation into the suspicion that
restaurants in less affluent neighborhoods tend to have higher rating variance
than those in more affluent neighborhoods.
* Methodology
This analysis uses the Yelp academic dataset, which consists of about 2.6
million user reviews, plus some relevant category data (~85k businesses, ~700k
users, etc). This data is stored natively in json, though the data is quite
flat, even more so one discounts some of the less useful bits of
information. Therefore the json was trimmed and converted to a csv format via a
python script, and the resulting csv files, uncompressed, total ~2 gigabytes on
disk. R is the language of choice for the analysis, with the data.table often
coming in handy for its speed.
* Initializing data 
Data has been preprocessed by a small Python script that flattens the source
json into csv and trims off useless data.
** Libraries :noexport:
Necessary libraries
#+BEGIN_SRC R :session :exports none :results none
  library(ggplot2)
  library(data.table)
  library(dplyr)
  library(ascii)
  options(asciiType = "org")
  options(max.print = 200)
#+END_SRC
** Data :ignore:
Need to load up about 2.3 gigabytes of tabular data and keep track of read
time. Thankfully ~fread~ from the the ~data.table~ package makes this process an
order of magnitude or two faster than R Base's ~read.csv~:
#+BEGIN_SRC R :session :exports none :cache no
  read_table <- function(filename) {                                          
      table <- fread(filename)  # use fread to quickly read csv file
      # Make sure there ren't any unacceptable chracters in the column names
      names(table) <- make.names(tolower(names(table)), unique = TRUE)
      table
  }

  print("Loading reviews...")
  reviews_t = system.time(reviews <- read_table('./data/review.csv'))

  print("Loading tip...")
  tips_t = system.time(tips <- read_table("./data/tip.csv"))

  print("Loading business...")
  business_t = system.time(business <- read_table("./data/business.csv"))

  print("Loading user...")
  users_t = system.time(users <- read_table("./data/user.csv"))

  print("Loading checkin...")
  checkins_t = system.time(checkins <- read_table("./data/checkin.csv"))
#+END_SRC

#+RESULTS[a89ba1709758873becea8679fe72f2880558629e]:

#+BEGIN_SRC R :session :exports results :results org
  total_load_time <- reviews_t + tips_t + business_t + users_t + checkins_t
  sprintf("Time to load CSV data into data.frames: %.2f minutes", total_load_time["elapsed"]/60.0)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Time to load CSV data into data.frames: 0.54 minutes
#+END_SRC

* Preprocessing
Zip code info for each business would be quite helpful for looking into the
demographical review trends. However, ZIP code info is not explicitly
required of businesses when registering with Yelp, so we have to see how easily
they can be extracted from from the 'full address' data for each business.
** ZIP codes
#+BEGIN_SRC R :session :exports none :results none
  grab_zip <- function(address) {
      as.numeric(substr(address,
                        nchar(address, keepNA = TRUE) - 4,
                        nchar(address, keepNA = TRUE)))
  }

  zips = lapply(business$full_address, grab_zip)

  business <- mutate(business, zip_codes = zips)
#+END_SRC
How many ZIPs did we get? Dang, turns out that only about 1/8 of the addresses
contain a retreivable zip code.
#+BEGIN_SRC R :session :exports none :results org
percent_null_zips <- length(zips[is.na(zips)])/length(zips)*100

sprintf("%.2f%% of restaurants have undecipherable zip codes", percent_null_zips)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
12.95% of restaurants have undecipherable zip codes
#+END_SRC

Upon closer inspection, it turns out that generally businesses do not provide
very good address information, and in fact, many do not provide any at
all. Luckily, Yelp maintains longitude/latitude coordinates of each business for
the purpose of Google maps integration, which we can confidently affirm with the
following assertion:
#+BEGIN_SRC R :session :exports both :results org
   longs <- grep('[[:digit:]]+.[[:digit:]]*', business$longitude)
   lats <- grep('[[:digit:]]+.[[:digit:]]*', business$latitude)
   stopifnot(length(longs) == length(lats),
             length(longs) == length(business$latitude))
   print("Done.")
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Done.
#+END_SRC
While geographical coordinates are readily at hand, it turns out that reverse
geocoding, the process of converting geographic coordinates to zip codes or the
like, costs money and is time consuming to do accurately. Instead, it could be
useful to get an idea of the quality of the rating inconsistency by instead
using restaurant price range, and not restaurant neighborhood median income (or
the like), as a feauture of interest.  It may be fun to at some point put in the
work/money to produce the zip code information.
** Additional categories and misc data cleaning :ignore:
#+BEGIN_SRC R :session :exports none :results  none
  business <- merge(business, 
                    rename(aggregate(stars ~ business_id,
                                     data=reviews,
                                     FUN=mean), 
                           stars.avg = stars),
                    by='business_id')
  business <- rename(business, stars.median = stars) # for pleasant merges with `reviews`
  business$price.range <- factor(business$price.range, labels=c('Low',
                                                                'Medium Low', 
                                                                'Medium High',
                                                                'High'))
#+END_SRC
* Distribution
** Inconsistency of ratings
 #+BEGIN_SRC R :session :exports code :results none :cache no
   star_variance <- merge(aggregate(stars ~ business_id,
                                    data = reviews, 
                                    FUN = var),
                          na.omit(business[,c('price.range',
                                              'stars.avg',
                                              'business_id',
                                              'review_count')]),
                          by = 'business_id')
   star_variance <- rename(star_variance, stars.var = stars)
 #+END_SRC
*** stars.var ~ price.range
   We can aggregate the businesses by price range and average out their rating
   variance to get an idea about how restaurant pricing correlates with reviewer
   consistency:
   #+NAME: star_var
   #+BEGIN_SRC R :session :exports both :colnames yes
     aggregate(stars.var ~ price.range, data = star_variance, FUN = mean)
   #+END_SRC

   #+RESULTS: star_var
   | price.range |        stars.var |
   |-------------+------------------|
   | Low         |  1.5834411360414 |
   | Medium Low  | 1.55586274965935 |
   | Medium High | 1.67594320395976 |
   | High        | 1.91272081281026 |

   It seems that the correlation between rating inconsistency and restaurant
   expensiveness is very small. Maybe instead of price range, we can look at
   rating average:

   #+NAME: variance_vs_rating
   #+BEGIN_SRC R :session :exports code :results org
     cor(star_variance$stars.var, star_variance$stars.avg, use='complete')
   #+END_SRC

   #+RESULTS: variance_vs_rating
   #+BEGIN_SRC org
   -0.447323849535184
   #+END_SRC

   #+BEGIN_SRC R :session :exports results :results org :var x=variance_vs_rating
     sprintf("Correlation between rating variance and rating average: %.2f", 
             as.numeric(x))
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   Correlation between rating variance and rating average: -0.45
   #+END_SRC

   So we've gotten a bit far at this point from the original idea of the effect
   of neighborhood demographics on rating consistency, but this might still be
   something worth exploring. We can look at the relationship between five-star
   and one-star vote rates for all businesses:

  #+NAME: star-dists
  #+BEGIN_SRC R :session :exports none :results silenced :cache no
    star_freq<- function(r, rating) {
        sum(r == rating)/length(r)
    }
    # There is definitely a nicer way to do this, but I'm done with that 
    # rabbit hole.
    s1 <- rename(aggregate(stars ~ business_id,
                           data=reviews,
                           FUN=function(stars) star_freq(stars, 1)),
                 one=stars)

    s2 <- rename(aggregate(stars ~ business_id,
                           data=reviews,
                           FUN=function(stars) star_freq(stars, 2)),
                 two=stars)

    s3 <- rename(aggregate(stars ~ business_id,
                           data=reviews,
                           FUN=function(stars) star_freq(stars, 3)),
                 three=stars)

    s4 <- rename(aggregate(stars ~ business_id,
                           data=reviews,
                           FUN=function(stars) star_freq(stars, 4)),
                 four=stars)

    s5 <- rename(aggregate(stars ~ business_id,
                           data=reviews,
                           FUN=function(stars) star_freq(stars, 5)),
                 five=stars)


    business <- merge(business, Reduce(merge,list(s1, s2, s3, s4, s5)),
                      by="business_id")
  #+END_SRC

  #+RESULTS[7bad3f915b246f2b57ed46b5f016196973dc16ff]: star-dists

  #+NAME: basic-star-var-graph
  #+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_LfYln761.png 
    library(scales)
    r <- filter(business, review_count > 100)
    ggplot(r, aes(x=one, y=five, color = price.range)) +
        geom_point() +
        scale_x_continuous(labels = percent) +
        scale_y_continuous(labels = percent) + 
        labs(color = "Business Price Range", 
             x = ("One star"),
             y = ("Five star"), 
             title="Rating composition: five-star vs one-star") 

  #+END_SRC

  #+RESULTS: basic-star-var-graph
  [[file:./img/R_LfYln761.png]]

  #+BEGIN_SRC R :session :exports none :results output org 
    g1 <- rename(aggregate(stars ~ business_id, data=reviews, FUN=function(stars) star_freq(stars, 1)), one=stars)
    g2 <- rename(aggregate(stars ~ business_id, data=reviews, FUN=function(stars) star_freq(stars, 2)), two=stars)
    g3 <- rename(aggregate(stars ~ business_id, data=reviews, FUN=function(stars) star_freq(stars, 3)), three=stars)
    g4 <- rename(aggregate(stars ~ business_id, data=reviews, FUN=function(stars) star_freq(stars, 4)), four=stars)
    g5 <- rename(aggregate(stars ~ business_id, data=reviews, FUN=function(stars) star_freq(stars, 5)), five=stars)

    business <- merge(business, Reduce(merge,list(g1, g2, g3, g4, g5)), by="business_id")
  #+END_SRC

  #+BEGIN_SRC R :session :exports both :results output org 
    star_freq<- function(rs) {   
        tabulate(rs)/length(rs)
    }

    business <- merge(rename(aggregate(stars ~ business_id,
                                       data=reviews,
                                       FUN=star_freq),
                             stars.dist=stars),
                      business,
                      by="business_id")
  #+END_SRC

*** stars.var ~ stars.avg
   So, we see that the rating variance for restaurants correlates negatively
   with their average ratings. That is, poorly rated restaurants have a tendency
   to also have more varied ratings. A possible conclusion is that restaurant
   ratings are simply skewed positively, and therefore deviation from mean
   rating is more often bounded at 5 stars than at 1 star (4 star restaurants
   will get occaisional 2-star ratings, but of course never a 6-star rating). We
   can investigate this idea with the following histographic depicting of the
   mean restaurant rating distribution. Note that we limit ourselves to those
   businesses with at least 20 reviews:

   #+BEGIN_SRC R :session :exports results :results graphics :file ./img/R_CCa0S6lS.png 
     b <- filter(business, review_count > 20)
     g <- ggplot(data=b, aes(stars.avg))
     g + geom_histogram(breaks=seq(1,5,by=.10),
                        fill="red",
                        col="red",
                        alpha=.2) + 
         labs(title = "Distribution average business rating", 
              x = "Mean Rating",
              y = "Count")
   #+END_SRC

#+RESULTS:
[[file:./img/R_CCa0S6lS.png]]
    
  Above we see that the average restaurant rating shows significant positive
  skew, and therefore the first hypothesis seems a bit more be believable. Most
  people who have experience with the internet, and therefore have some
  familiarity with online rating systems like Yelp, probably have an intuitive
  idea about this tendency for these ratings systems to have a very strong
  positive skew. I suspect a major reason for this is perceived social pressure,
  particularly in the case of a Facebook-driven website such as Yelp ([[http://sloanreview.mit.edu/article/the-problem-with-online-ratings-2/][which is
  not a unique idea]]). For reference, the mean business rating is a rather high
  src_R[:session]{sprintf("%.2f", mean(business$stars.avg))} {{{results(=3.69=)}}} stars.

*** stars.avg ~ price.range                                          :ignore:
    In any case, we might still wonder why there exists this correlation between
    rating inconsistency and average rating, yet also no such correlation
    whatsoever between rating inconsistency and restaurant expensiveness,
    statements which we might expect, possibly naively, to be quite similar
    (i.e., more expensive restaurants are generally rated more highly). The
    reason for this lack of correlation is because this is indeed a naive
    assumption:

#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_Sr5sdYpc.png 
  ggplot(business, aes(x=price.range, y=stars.avg, fill=price.range)) + 
      geom_boxplot() + 
      stat_summary(fun.y="mean", geom="point") + 
      labs(x = "Price Range",
           y = "Rating average",
           title = "Rating distribution by price ranges")
#+END_SRC

#+RESULTS:
[[file:./img/R_Sr5sdYpc.png]]

I suspect an explanation for this indescrepency is simply that the value to
which these ratings refer is not very well in line with what we, as consumers,
intuitively and automatically summarize them to mean. So, while as a consumer we
think of these ratings, without much actual precise consideration, as a general
measure of "goodness", with zero being un-good and five being very good, as
reviewers we (the collective "we") are likely to make all of the considerations
required for an accurate evaluation (e.g., average restaurant goodness, pricing,
etc). Indeed, it might be more accurate to describe my own system as how little
my satisfaction with the restaurant deviated from my expected experience. In this
way, I've normalized my perspective on the restaurant, but without actually
yielding me any bang-for-the-buck measure. I consider this a bad and unhelpful
way to contribute my opinion on the business, but this is the way that I feel I
am most naturally inclined.
** Price distribution
The pricing makeup of our positively skewed restaurant rating distribution is
not particularly surprising:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_YzrIrkYy.png 
  # priced restaurants only
  ggplot(business[!is.na(business$price.range), ],
         aes(x=stars.avg, fill=price.range)) + geom_histogram(binwidth=.25) +
         ylab('Count') +
         xlab('Rating average (mean)') +
         labs(fill="Price Range") +
         ggtitle('Distribution of ratings by business price range')
#+END_SRC

#+RESULTS:
[[file:./img/R_YzrIrkYy.png]]

Interestingly, it seems that unpriced restaurants, i.e. restaurants for which a
price range has not yet been assigned via user concensus, are, however, not only
considerably more positively rated, but also in a seemingly linear fashion:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_vvM4L9Z2.png 
  b <- business[business$review_count > 20, ]
  ggplot(b[is.na(b$price.range),], aes(x=stars.avg)) +
      geom_histogram(binwidth=.10, color='orange', fill='orange') +
      ylab('Count') +
      xlab('Rating average (mean)') +
      labs(fill="Price Range") +
      ggtitle('Distribution of ratings for unpriced businesses by price range')
#+END_SRC

#+RESULTS:
[[file:./img/R_vvM4L9Z2.png]]

We can see clearly that there is a much more siginificant positive skew for
these unrated restaurants. This begs the question, is there a downward tendency
for restaurant ratings as their profiles mature? The fact that unrated
restaurants tend to be less those with less mature profiles is glaringly
suspicious.

** TODO Sample businesses by first 20 or so reviews (chronologically)
- See if they have a similar distribution to this rating average stuff
** REST
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_3EqwcmXp.png 
s <- star_variance[star_variance$review_count > 20, ]
ggplot(s, aes(x=stars.var)) + geom_histogram(color='red', fill='red', binwidth=.1)
#+END_SRC

#+RESULTS:
[[file:./img/R_3EqwcmXp.png]]

#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_wGL1DyI7.png 
  s <- star_variance[star_variance$review_count > 100, ]
  ggplot(s, aes(x=stars.var, y=review_count)) + geom_point()
#+END_SRC

#+RESULTS:
[[file:./img/R_wGL1DyI7.png]]

*** Basic stuff
 #+BEGIN_SRC R :session :exports results :results org
   sprintf("Average rating across all reviews: %.3f", mean(reviews$stars))
 #+END_SRC

 #+RESULTS: 
 #+BEGIN_SRC org
 Average rating across all reviews: 3.764
 #+END_SRC

* BIN
We can see the law of large numbers in action
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_vQgMpNec.png 
  s <- sample_n(filter(star_variance, review_count > 30 ), 16000)
  ggplot(filter(s, as.numeric(s$price.range) == 1), aes(x=review_count, y=stars.var)) + 
      geom_point() + 
      scale_y_continuous(limits = c(0, 4)) + 
      scale_x_continuous(limits = c(0, 4000))
#+END_SRC

#+RESULTS:
[[file:./img/R_vQgMpNec.png]]


