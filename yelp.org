#+HTML_HEAD: <link href="/home/dodge/.emacs.d/leuven-theme.css" rel="stylesheet">
#+TITLE: *@@html:<font color = "C2492F">@@Sensibility of Yelp Rating Trends@@html:</font>@@*

#+OPTIONS: toc:2 num:nil

* Initializing data 
Data has been preprocessed by a small Python script that flattens the source
json into csv and trims off useless data.
** Libraries :noexport:
Necessary libraries
#+BEGIN_SRC R :session :exports none :results none
  library(ggplot2)
  library(data.table)
  library(dplyr)
  library(ascii)
  options(asciiType = "org")
  options(max.print = 200)
#+END_SRC
** Data :ignore:
Need to load up about 2.3 gigabytes of tabular data and keep track of read
time. Thankfully ~fread~ from the the ~data.table~ package makes this process an
order of magnitude or two faster than R Base's ~read.csv~:
#+BEGIN_SRC R :session :exports none :cache yes
  read_table <- function(filename) {                                          
      table <- fread(filename)  # use fread to quickly read csv file
      # Make sure there ren't any unacceptable chracters in the column names
      names(table) <- make.names(tolower(names(table)), unique = TRUE)
      table
  }

  print("Loading reviews...")
  reviews_t = system.time(reviews <- read_table('./data/review.csv'))

  print("Loading tip...")
  tips_t = system.time(tips <- read_table("./data/tip.csv"))

  print("Loading business...")
  business_t = system.time(business <- read_table("./data/business.csv"))

  print("Loading user...")
  users_t = system.time(users <- read_table("./data/user.csv"))

  print("Loading checkin...")
  checkins_t = system.time(checkins <- read_table("./data/checkin.csv"))
#+END_SRC

#+RESULTS[a89ba1709758873becea8679fe72f2880558629e]:

#+BEGIN_SRC R :session :exports results :results org
  total_load_time <- reviews_t + tips_t + business_t + users_t + checkins_t
  sprintf("Time to load CSV data into data.frames: %.2f minutes", total_load_time["elapsed"]/60.0)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Time to load CSV data into data.frames: 0.20 minutes
#+END_SRC

* Introduction
* Methodology
* Preprocessing
Zip code info for each business would be quite helpful for doing some looking
into demographical patronage trends. However, ZIP code info is not explicitly
required of businesses when registering with Yelp, so we have to see how easily
they can be extracted from from the 'full address' data:
** ZIP codes
#+BEGIN_SRC R :session :exports none :results none
  grab_zip <- function(address) {
      as.numeric(substr(address,
                        nchar(address, keepNA = TRUE) - 4,
                        nchar(address, keepNA = TRUE)))
  }

  zips = lapply(business$full_address, grab_zip)

  business <- mutate(business, zip_codes = zips)
#+END_SRC
How many ZIPs did we get? Dang, turns out that the about 1/8 of the addresses
did not contain zipcodes easily grabbed.
#+BEGIN_SRC R :session :exports none :results org
percent_null_zips <- length(zips[is.na(zips)])/length(zips)*100

sprintf("%.2f%% of restaurants have undecipherable zip codes", percent_null_zips)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
12.61% of restaurants have undecipherable zip codes
#+END_SRC

Upon closer inspection, it is apparent that generally businesses do not provide
very good address information, and in fact, many do not provide any at
all. Luckily Yelp maintains longitude/latitude coordinates of each business for
the purpose of Google maps integration, which we can confidently affirm with the
following assertion:
#+BEGIN_SRC R :session :exports both :results org
   longs <- grep('[[:digit:]]+.[[:digit:]]*', business$longitude)
   lats <- grep('[[:digit:]]+.[[:digit:]]*', business$latitude)
   stopifnot(length(longs) == length(lats),
             length(longs) == length(business$latitude))
   print("Done.")
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Done.
#+END_SRC

While geographical coordinates are readily at hand, reverse geocoding, the
process of converting geographic coordinates to zip codes or the like, is
expensive and time consuming to do accurately. Instead, we can get away with
just looking at the price range states for individual restaurants, which is much
easier. It may be interesting to at some point put in the work/money to produce
the zip code information.
** Additional categories and misc data cleaning :ignore:
#+BEGIN_SRC R :session :exports none :results  none
  business <- merge(business, 
                    rename(aggregate(stars ~ business_id,
                                     data=reviews,
                                     FUN=mean), 
                           stars.avg = stars),
                    by='business_id')
  business <- rename(business, stars.median = stars) # for pleasant merges with `reviews`
  business$price.range <- factor(business$price.range, labels=c('Low',
                                                                'Medium Low', 
                                                                'Medium High',
                                                                'High'))
#+END_SRC
* Distribution
** Rating variance
 #+BEGIN_SRC R :session :exports code :results none :cache yes
   star_variance <- merge(aggregate(stars ~ business_id,
                                    data = reviews, 
                                    FUN = var),
                          na.omit(business[,c('price.range',
                                              'stars.avg',
                                              'business_id',
                                              'review_count')]),
                          by = 'business_id')
   star_variance <- rename(star_variance, stars.var = stars)
 #+END_SRC
*** stars.var ~ price.range
   Because deriving demographics from geographical coordinates is a bit
   difficult, it might be useful to get an idea of the quality of the rating
   inconsistency by instead using restaurant price range, and not restaurant
   neighborhood median income (or the like), as a feauture of interest. We can
   aggregate the businesses by price range and average out their rating variance
   to get an idea about this:

   #+NAME: stars_pr
   #+BEGIN_SRC R :session :exports code :colnames yes 
     aggregate(stars.var ~ price.range, data = star_variance, FUN = mean)
   #+END_SRC

   #+RESULTS: stars_pr
   | price.range |        stars.var |
   |-------------+------------------|
   | Low         |  1.5834411360414 |
   | Medium Low  | 1.55586274965935 |
   | Medium High | 1.67594320395976 |
   | High        | 1.91272081281026 |

   It seems that the correlation between rating inconsistency and restaurant
   expensiveness is very small. Maybe instead of price range, we can look at
   rating average:

   #+NAME: variance_vs_rating
   #+BEGIN_SRC R :session :exports code :results org
     cor(star_variance$stars.var, star_variance$stars.avg, use='complete')
   #+END_SRC

   #+RESULTS: variance_vs_rating
   #+BEGIN_SRC org
   -0.447323849535184
   #+END_SRC

   #+BEGIN_SRC R :session :exports results :results org :var x=variance_vs_rating
     sprintf("Correlation between rating variance and rating average: %.2f", 
             as.numeric(x))
   #+END_SRC

   #+RESULTS:
   #+BEGIN_SRC org
   Correlation between rating variance and rating average: -0.45
   #+END_SRC
   
*** stars.var ~ stars.avg
   So, we see that the rating variance for restaurants correlates negatively
   with their average ratings. That is, poorly rated restaurants have a tendency
   to also have more varied ratings. A possible conclusion is that restaurant
   ratings are simply skewed positively, and therefore deviation from mean
   rating is more often bounded at 5 stars than at 1 star (4 star restaurants
   will get occaisional 2-star ratings, but of course never a 6-star rating). We
   can investigate this idea with the following histographic depicting of
   the mean restaurant rating distribution:

   #+BEGIN_SRC R :session :exports results :results graphics :file ./img/R_CCa0S6lS.png 
     b <- filter(business, review_count > 20)
     g <- ggplot(data=b, aes(stars.avg))
     g + geom_histogram(breaks=seq(1,5,by=.10),
                        fill="red",
                        col="red",
                        alpha=.2) + 
         labs(title = "Distribution average business rating", 
              x = "Mean Rating",
              y = "Count")
   #+END_SRC

#+RESULTS:
[[file:./img/R_CCa0S6lS.png]]
    
  Above we see that the average restaurant rating shows significant positive
  skew, and therefore the first hypothesis seems a bit more be believable. most
  people who have experience with the internet, and therefore have some
  familiarity with online rating systems like Yelp, probably have an intuitive
  idea about this tendency for these ratings systems to have a very strong
  positive skew. I suspect a major reason for this is social pressure, a factor
  that is particularly present on a Facebook-based website like Yelp ([[http://sloanreview.mit.edu/article/the-problem-with-online-ratings-2/][which is
  not a unique idea]]). For reference, the mean restaurant rating is a rather high
  src_R[:session]{sprintf("%.2f", mean(business$stars.avg))}
  {{{results(=3.69=)}}} stars.

*** stars.avg ~ price.range                                          :ignore:
    In any case, we might still wonder why there exists this correlation between
    rating inconsistency and average rating, yet also no such correlation
    whatsoever between rating inconsistency and restaurant expensiveness,
    statements which we might expect, possibly naively, to be very similar
    (i.e., more expensive restaurants are generally rated more highly). The
    reason for this lack of correlation is because this is indeed a naive
    assumption:

#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_JTmgqG9.png 
  b <- aggregate(stars.avg ~ price.range, data=business, FUN=mean)
  ggplot(b, aes(x=price.range, y=stars.avg)) + 
      geom_bar(stat='identity', color='black', fill='yellow', alpha=.2) +
      scale_y_continuous(limits = c(0, 5)) + 
      xlab('Price Range') +
      ylab('Average rating (stars)') + 
      ggtitle('Business Price Range vs Rating')
#+END_SRC

#+RESULTS:
[[file:./img/R_JTmgqG9.png]]

** Price distribution
The pricing makeup of our positively skewed restaurant rating distribution is
not particularly surprising:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_YzrIrkYy.png 
  # priced restaurants only
  ggplot(business[!is.na(business$price.range), ],
         aes(x=stars.avg, fill=price.range)) + geom_histogram(binwidth=.5)
#+END_SRC

#+RESULTS:
[[file:./img/R_YzrIrkYy.png]]

Interestingly, it seems that unpriced restaurants, i.e., restaurants for which a
price range has not been assigned via user concensus, are, however, considerably more
positively rated:
#+BEGIN_SRC R :session :exports results :results output graphics :file ./img/R_vvM4L9Z2.png 
  ggplot(business[is.na(business$price.range),], aes(x=stars.avg)) +
      geom_histogram(binwidth=.5, color='orange', fill='orange')
#+END_SRC

#+RESULTS:
[[file:./img/R_vvM4L9Z2.png]]

This begs the question, is there a downward tendency for restaurant ratings as
the profiles for these restaurants mature?
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_3EqwcmXp.png 
ggplot(star_variance, aes(x=stars)) + geom_histogram(color='red', fill='red')
#+END_SRC

#+RESULTS:
[[file:./img/R_3EqwcmXp.png]]

*** Basic stuff
 #+BEGIN_SRC R :session :exports results :results org
   sprintf("Average rating across all reviews: %.3f", mean(reviews$stars))
 #+END_SRC

 #+RESULTS:
 #+BEGIN_SRC org
 Average rating across all reviews: 3.764
 #+END_SRC

** Distribution of scores by pricing 

#+BEGIN_SRC R :session :exports results :results none
  bus <- business[,c('price.range', 'stars.avg', 'business_id', 'review_count')]
#+END_SRC

#+BEGIN_SRC R :session :exports code :results none :cache yes
  bus_reviews <- merge(na.omit(bus), reviews, by = 'business_id')
#+END_SRC

#+BEGIN_SRC R :session :exports code :results org :cache yes
  print(ascii(aggregate(bus_reviews, 
                        by = list(bus_reviews$price.range),
                        FUN = var)))
#+END_SRC

#+RESULTS[df58e5b91980b445999fd6612031ba52aeea7157]:
#+BEGIN_SRC org
#+END_SRC


#+BEGIN_SRC R :session :exports both :results none
# review counts for businesses with and without listed price range
mean_no_pr_rev_count <- mean(bus[is.na(bus$price.range)]$review_count)
mean_pr_rev_count <- mean(bus[!is.na(bus$price.range)]$review_count)
#+END_SRC
* BIN
We can see the law of large numbers in action
#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_vQgMpNec.png 
  s <- sample_n(filter(star_variance, review_count > 30 ), 16000)
  ggplot(filter(s, as.numeric(s$price.range) == 1), aes(x=review_count, y=stars.var)) + 
      geom_point() + 
      scale_y_continuous(limits = c(0, 4)) + 
      scale_x_continuous(limits = c(0, 4000))
#+END_SRC

#+RESULTS:
[[file:./img/R_vQgMpNec.png]]


#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/R_8bbjovom.png 
  s <- sample_n(filter(star_variance, review_count > 30 ), 16000)
  ggplot(business, aes(x=review_count, y=stars.avg)) + 
      geom_point() + 
      scale_y_continuous(limits = c(0, 4)) + 
      scale_x_continuous(limits = c(0, 4000))
#+END_SRC

#+RESULTS:
[[file:./img/R_8bbjovom.png]]
